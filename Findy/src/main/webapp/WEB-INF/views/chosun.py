# í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ ë‹ˆì½”ë‹ˆì½”ë‹ˆ~

import time 
 # ëŒ€ê¸° ì‹œê°„ ì œì–´ìš©
import threading  
# ìŠ¤ë ˆë“œ ê°„ ë™ê¸°í™”ì— ì‚¬ìš©
from concurrent.futures import ThreadPoolExecutor  
# ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPool
from selenium import webdriver  
# ì›¹ í˜ì´ì§€ ì¡°ì‘ì„ ìœ„í•œ Selenium
from selenium.webdriver.chrome.options import Options  
# Chrome ì˜µì…˜ ì„¤ì •
from bs4 import BeautifulSoup 
 # HTML íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import urljoin, urlparse  
# URL ë³‘í•© ë° íŒŒì‹±ìš©
from pymongo import MongoClient 
 # MongoDBì™€ ì—°ë™
from datetime import datetime 
 # í˜„ì¬ ì‹œê° ê¸°ë¡ìš©

# ======================
# [1] Selenium ë“œë¼ì´ë²„ ì„¤ì • í•¨ìˆ˜ ì •ì˜
# ======================
def get_driver():
    options = Options()  # Chrome ì‹¤í–‰ ì˜µì…˜ ê°ì²´ ìƒì„±
    options.add_argument("--headless=new")  # ë¸Œë¼ìš°ì €ë¥¼ ë°±ê·¸ë¼ìš´ë“œ(í™”ë©´ ì—†ì´) ì‹¤í–‰
    options.add_argument("--no-sandbox")  # ë³´ì•ˆ ìƒŒë“œë°•ìŠ¤ í•´ì œ (ë¦¬ëˆ…ìŠ¤ì—ì„œ í•„ìš”)
    options.add_argument("--disable-dev-shm-usage")  # ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš© ì œí•œ í•´ì œ
    # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì •: ì„œë²„ì—ì„œ ë´‡ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(options=options)  # ì„¤ì •ëœ ì˜µì…˜ìœ¼ë¡œ Chrome ë“œë¼ì´ë²„ ì‹¤í–‰

# ======================
# [2] ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë§í¬ ì¶”ì¶œ
# ======================
def get_category_links():
    url = "https://www.chosun.com/"  # ì¡°ì„ ì¼ë³´ ë©”ì¸ ì£¼ì†Œ
    driver = get_driver()  # ì›¹ë“œë¼ì´ë²„ ìƒì„±
    category_links = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set

    try:
        driver.get(url)  # ë©”ì¸ í˜ì´ì§€ ìš”ì²­
        soup = BeautifulSoup(driver.page_source, "html.parser")  # HTML íŒŒì‹±

        # a íƒœê·¸ë¥¼ ëª¨ë‘ ê²€ì‚¬í•˜ì—¬ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ë§í¬ë§Œ í•„í„°ë§
        for a_tag in soup.select("a"):
            href = a_tag.get("href")
            if href and href.startswith("/") and any(cat in href for cat in [
                "/politics/", "/economy/", "/sports/", "/culture/",
                "/international/", "/national/", "/opinion/",
                "/entertainments/", "/game/", "/health/", "/sisa/"
            ]):
                # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜ í›„ ì¤‘ë³µ ì œê±°
                full_url = urljoin(url, href.split("?")[0].split("#")[0])
                category_links.add(full_url)

    except Exception as e:
        print(f"[ì—ëŸ¬] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")  # ì˜ˆì™¸ ë°œìƒ ì‹œ ë©”ì‹œì§€ ì¶œë ¥
    finally:
        driver.quit()  # ë“œë¼ì´ë²„ ì¢…ë£Œ

    return list(category_links)  # set â†’ list ë³€í™˜ í›„ ë°˜í™˜

# ======================
# [3] ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë³¸ë¬¸, ë‚ ì§œ ë“± ì¶”ì¶œ
# ======================
def extract_article_data(driver, article_url, category_name):
    try:
        driver.get(article_url)  # ê¸°ì‚¬ í˜ì´ì§€ ìš”ì²­
        time.sleep(1)  # ë¡œë”© ëŒ€ê¸°
        soup = BeautifulSoup(driver.page_source, "html.parser")  # HTML íŒŒì‹±

        # ì œëª© ì¶”ì¶œ (og:title ë©”íƒ€ íƒœê·¸ ì´ìš©)
        title_tag = soup.select_one("meta[property='og:title']")
        title = title_tag["content"] if title_tag else ""

        # ë³¸ë¬¸ ì¶”ì¶œ (í‘œì¤€ êµ¬ì¡° ê¸°ì¤€)
        content = ""
        section = soup.select_one("section.article-body[itemprop='articleBody']")
        if section:
            paragraphs = section.find_all("p")
            content = "\n".join(p.get_text(strip=True) for p in paragraphs)

        # êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš° ë°±ì—… ì„ íƒì ì‚¬ìš©
        if not content:
            fallback = soup.select_one("div.article-body") or soup.select_one("div#article-body")
            if fallback:
                paragraphs = fallback.find_all("p")
                content = "\n".join(p.get_text(strip=True) for p in paragraphs)

        # ë°œí–‰ì¼ ì¶”ì¶œ (ISO í˜•ì‹ ë¬¸ìì—´)
        pub_time = soup.select_one("meta[property='article:published_time']")
        published_at = pub_time["content"] if pub_time else None

        # URLì—ì„œ post_id ì¶”ì¶œ (ì¤‘ë³µ ì œê±°ìš© í‚¤)
        post_id = urlparse(article_url).path.rstrip("/").split("/")[-1]

        return {
            "title": title,
            "link": article_url,
            "content": content,
            "post_id": post_id,
            "published_at": published_at,
            "scraped_at": datetime.now(),  # ìˆ˜ì§‘ ì‹œê°
            "category": category_name
        }

    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨] {article_url} â†’ {e}")
        return None  # ì—ëŸ¬ ë°œìƒ ì‹œ None ë°˜í™˜

# ======================
# [4] í•œ ì¹´í…Œê³ ë¦¬ ë‚´ ê¸°ì‚¬ ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘
# ======================
def collect_articles_from_category(category_url, max_pages=3):
    category_name = category_url.strip("/").split("/")[-1]  # URLì—ì„œ ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ
    collected = []  # ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
    driver = get_driver()  # ë“œë¼ì´ë²„ ìƒì„±

    try:
        for page in range(1, max_pages + 1):  # í˜ì´ì§€ ë°˜ë³µ
            page_url = f"{category_url}?page={page}"
            print(f"[ì¹´í…Œê³ ë¦¬] {category_name} â†’ {page_url}")

            driver.get(page_url)
            time.sleep(1)
            soup = BeautifulSoup(driver.page_source, "html.parser")

            # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
            links = soup.select("a.story-card__headline")
            if not links:
                print(" ê¸°ì‚¬ ì—†ìŒ, ì¤‘ë‹¨")
                break

            for a_tag in links:
                href = a_tag.get("href")
                if not href:
                    continue
                article_url = urljoin(category_url, href)  # ì ˆëŒ€ê²½ë¡œ
                article_data = extract_article_data(driver, article_url, category_name)
                if article_data:
                    collected.append(article_data)

    except Exception as e:
        print(f"[í˜ì´ì§€ ì˜¤ë¥˜] {category_url} â†’ {e}")
    finally:
        driver.quit()

    return collected  # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ ìˆ˜ì§‘í•œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

# ======================
# [5] MongoDBì— ì €ì¥
# ======================
def save_to_mongodb(articles):
    try:
        client = MongoClient("mongodb://localhost:27017/")  # MongoDB ì—°ê²°
        db = client["newsdata"]  # DB ì„ íƒ
        collection = db["newsdata"]  # ì»¬ë ‰ì…˜ ì„ íƒ

        total = len(articles)
        unique_post_ids = set()
        inserted = 0
        skipped = 0

        for article in articles:
            post_id = article.get("post_id")
            if not post_id:
                skipped += 1
                continue

            unique_post_ids.add(post_id)
#################ì—¬ê¸°ì„œê°€ ì¤‘ë³µ ì œê±°ìš© ë¡œì§ì„ ##########################

            # ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ í›„ upsert (ì—†ìœ¼ë©´ insert, ìˆìœ¼ë©´ update)
            result = collection.update_one(
                {"post_id": post_id},
                {"$set": article},
                upsert=True
            )
            if result.upserted_id or result.modified_count:
                inserted += 1

        # ê²°ê³¼ ì¶œë ¥
        print(f"\n ì „ì²´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {total}ê±´")
        print(f" ê³ ìœ  post_id ìˆ˜: {len(unique_post_ids)}ê±´")
        print(f" ì €ì¥ ë˜ëŠ” ê°±ì‹ : {inserted}ê±´")
        print(f" post_id ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µëœ ê±´ìˆ˜: {skipped}ê±´")

    except Exception as e:
        print(f"[MongoDB ì˜¤ë¥˜] {e}")

# ======================
# [6] ë©”ì¸ í•¨ìˆ˜: ë³‘ë ¬ ìˆ˜ì§‘ ë° ì €ì¥ ì‹¤í–‰ ( ì´ê±°ì‹œ ë©€í‹° ì“°ë ˆë”©)
# ======================
if __name__ == "__main__":
    categories = get_category_links()  # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë§í¬ ìˆ˜ì§‘
    print(f"\nğŸ“š ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ: {len(categories)}ê°œ")

    total_articles = []  # ì „ì²´ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    lock = threading.Lock()  # ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼ ì‹œ ë™ê¸°í™”ìš© ë½

    # ê° ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ í•¨ìˆ˜ (ìŠ¤ë ˆë“œìš©)
    def process_category(cat_url):
        articles = collect_articles_from_category(cat_url, max_pages=3)
        print(f"  {cat_url} â†’ {len(articles)}ê±´ ìˆ˜ì§‘ë¨")
        with lock:  # ìŠ¤ë ˆë“œ ê°„ ì¶©ëŒ ë°©ì§€
            total_articles.extend(articles)

    # ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ìµœëŒ€ 5ê°œ ìŠ¤ë ˆë“œ)
    with ThreadPoolExecutor(max_workers=5) as executor:
        executor.map(process_category, categories)

    print(f"\nğŸ“° ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(total_articles)}")
    save_to_mongodb(total_articles)  # MongoDB ì €ì¥