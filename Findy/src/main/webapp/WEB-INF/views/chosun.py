# í•„ìš”í•œ ëª¨ë“ˆë“¤ì„ ë‹ˆì½”ë‹ˆì½”ë‹ˆ~

import time  # ëŒ€ê¸° ì‹œê°„ ì œì–´ìš©
import threading  # ìŠ¤ë ˆë“œ ê°„ ë™ê¸°í™”ì— ì‚¬ìš©
from concurrent.futures import ThreadPoolExecutor  # ë³‘ë ¬ì²˜ë¦¬ë¥¼ ìœ„í•œ ThreadPool
from selenium import webdriver  # ì›¹ í˜ì´ì§€ ì¡°ì‘ì„ ìœ„í•œ Selenium
from selenium.webdriver.chrome.options import Options  # Chrome ì˜µì…˜ ì„¤ì •
from bs4 import BeautifulSoup  # HTML íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬
from urllib.parse import urljoin, urlparse  # URL ë³‘í•© ë° íŒŒì‹±ìš©
from pymongo import MongoClient  # MongoDBì™€ ì—°ë™
from datetime import datetime  # í˜„ì¬ ì‹œê° ê¸°ë¡ìš©


# ======================
# ì¹´í…Œê³ ë¦¬ ë¯¸ë¦¬ ì„ ì–¸í•´ì£¼ì 
# ======================
CATEGORY_MAP = {
    "economy": "ê²½ì œ",
    "opinion": "ì˜¤í”¼ë‹ˆì–¸",
    "national": "ì‚¬íšŒ",
    "health": "ê±´ê°•",
    "sports": "ìŠ¤í¬ì¸ ",
    "culture": "ë¬¸í™”/ì—°ì˜ˆ"
}

ALLOWED_CATEGORIES = set(CATEGORY_MAP.keys())


# ======================
# [1] Selenium ë“œë¼ì´ë²„ ì„¤ì • í•¨ìˆ˜ ì •ì˜
# ======================
def get_driver():
    options = Options()  # Chrome ì‹¤í–‰ ì˜µì…˜ ê°ì²´ ìƒì„±
    options.add_argument("--headless=new")  # ë¸Œë¼ìš°ì €ë¥¼ ë°±ê·¸ë¼ìš´ë“œ(í™”ë©´ ì—†ì´) ì‹¤í–‰
    options.add_argument("--no-sandbox")  # ë³´ì•ˆ ìƒŒë“œë°•ìŠ¤ í•´ì œ (ë¦¬ëˆ…ìŠ¤ì—ì„œ í•„ìš”)
    options.add_argument("--disable-dev-shm-usage")  # ê³µìœ  ë©”ëª¨ë¦¬ ì‚¬ìš© ì œí•œ í•´ì œ
    options.add_argument(
        # ì‚¬ìš©ì ì—ì´ì „íŠ¸ ì„¤ì •: ì„œë²„ì—ì„œ ë´‡ ì°¨ë‹¨ì„ í”¼í•˜ê¸° ìœ„í•´ ì¼ë°˜ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
    )
    return webdriver.Chrome(options=options)  # ì„¤ì •ëœ ì˜µì…˜ìœ¼ë¡œ Chrome ë“œë¼ì´ë²„ ì‹¤í–‰

# ======================
# [2] ë©”ì¸ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë§í¬ ì¶”ì¶œ
# ======================
def get_category_links():
    url = "https://www.chosun.com/"  # ì¡°ì„ ì¼ë³´ ë©”ì¸ ì£¼ì†Œ
    driver = get_driver()  # ì›¹ë“œë¼ì´ë²„ ìƒì„±
    category_links = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ set

    try:
        driver.get(url)  # ë©”ì¸ í˜ì´ì§€ ìš”ì²­
        soup = BeautifulSoup(driver.page_source, "html.parser")  # HTML íŒŒì‹±

        # a íƒœê·¸ë¥¼ ëª¨ë‘ ê²€ì‚¬í•˜ì—¬ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ë§í¬ë§Œ í•„í„°ë§
        for a_tag in soup.select("a"):
            href = a_tag.get("href")
            if href and href.startswith("/") and any(cat in href for cat in [
                "/politics/", "/economy/", "/sports/", "/culture/",
                "/international/", "/national/", "/opinion/",
                "/entertainments/", "/game/", "/health/", "/sisa/"
            ]):
                full_url = urljoin(url, href.split("?")[0].split("#")[0])  # ì ˆëŒ€ê²½ë¡œë¡œ ì •ì œ
                category_links.add(full_url)  # ì¤‘ë³µ ì—†ì´ ì €ì¥

    except Exception as e:
        print(f"[ì—ëŸ¬] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
    finally:
        driver.quit()  # ë“œë¼ì´ë²„ ì¢…ë£Œ

    return list(category_links)  # set â†’ list ë³€í™˜ í›„ ë°˜í™˜

# ======================
# [3] ê¸°ì‚¬ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì œëª©, ë³¸ë¬¸, ë‚ ì§œ ë“± ì¶”ì¶œ
# ======================
def extract_article_data(driver, article_url, category_name):
    try:
        driver.get(article_url)  # ê¸°ì‚¬ í˜ì´ì§€ ìš”ì²­
        time.sleep(1)  # ë¡œë”© ëŒ€ê¸°
        soup = BeautifulSoup(driver.page_source, "html.parser")  # HTML íŒŒì‹±

        title_tag = soup.select_one("meta[property='og:title']")  # ì œëª© ì¶”ì¶œìš© ë©”íƒ€ íƒœê·¸
        title = title_tag["content"] if title_tag else ""

        content = ""  # ê¸°ì‚¬ ë³¸ë¬¸ ì´ˆê¸°í™”
        section = soup.select_one("section.article-body[itemprop='articleBody']")  # ë³¸ë¬¸ ì„¹ì…˜ ì°¾ê¸°
        if section:
            paragraphs = section.find_all("p")  # <p> ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            content = "\n".join(p.get_text(strip=True) for p in paragraphs)

        if not content:  # ë°±ì—… ì„ íƒì ì‚¬ìš©
            fallback = soup.select_one("div.article-body") or soup.select_one("div#article-body")
            if fallback:
                paragraphs = fallback.find_all("p")
                content = "\n".join(p.get_text(strip=True) for p in paragraphs)

        pub_time = soup.select_one("meta[property='article:published_time']")  # ë°œí–‰ì¼ ì¶”ì¶œ
        published_at = pub_time["content"] if pub_time else None

        # post_id = urlparse(article_url).path.rstrip("/").split("/")[-1]  # URLì—ì„œ post_id ì¶”ì¶œ
        post_id = article_url  # ê± URLì„ post_id ëŠë‚Œìœ¼ë¡œ ì‚¬ìš© 

        return {
            "headline": title,
            "url": article_url,
            "content": content,
            "url": post_id,
            "time": published_at,
            # "scraped_at": datetime.now(),  # ìˆ˜ì§‘ ì‹œê°
            "category": category_name,
            "source": "chosun"
        }

    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨] {article_url} â†’ {e}")
        return None

# ======================
# [4] í•œ ì¹´í…Œê³ ë¦¬ ë‚´ ê¸°ì‚¬ ì—¬ëŸ¬ í˜ì´ì§€ ìˆ˜ì§‘
# ======================
def collect_articles_from_category(category_url, max_pages=3):
    path = urlparse(category_url).path
    category_key = path.strip("/").split("/")[0]  # "economy" ë“± ì¶”ì¶œ

    if category_key not in CATEGORY_MAP:
        print(f"[ìŠ¤í‚µ] '{category_url}'ì€ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ê°€ ì•„ë‹˜")
        return []

    category_name = CATEGORY_MAP[category_key]
    collected = []
    driver = get_driver()

    try:
        for page in range(1, max_pages + 1):
            page_url = f"{category_url}?page={page}"
            print(f"[ì¹´í…Œê³ ë¦¬] {category_name} â†’ {page_url}")

            driver.get(page_url)
            time.sleep(1)
            soup = BeautifulSoup(driver.page_source, "html.parser")

            links = soup.select("a.story-card__headline")
            if not links:
                print(" ê¸°ì‚¬ ì—†ìŒ, ì¤‘ë‹¨")
                break

            for a_tag in links:
                href = a_tag.get("href")
                if not href:
                    continue
                article_url = urljoin(category_url, href)
                article_data = extract_article_data(driver, article_url, category_name)
                if article_data:
                    collected.append(article_data)

    except Exception as e:
        print(f"[í˜ì´ì§€ ì˜¤ë¥˜] {category_url} â†’ {e}")
    finally:
        driver.quit()

    return collected

# ======================
# [5] MongoDBì— ì €ì¥
# ======================
def save_to_mongodb(articles):
    try:
        client = MongoClient("mongodb://localhost:27017/")  # MongoDB ì—°ê²°
        db = client["newsdata"]
        collection = db["newsdata"]

        total = len(articles)
        unique_post_ids = set()
        inserted = 0
        skipped = 0

        for article in articles:
            post_id = article.get("post_id")
            if not post_id:
                skipped += 1
                continue

            unique_post_ids.add(post_id)  # post_id ì¤‘ë³µ ë°©ì§€ìš©

            # ì¤‘ë³µ ì—¬ë¶€ í™•ì¸ í›„ upsert
            result = collection.update_one(
                {"post_id": post_id},
                {"$set": article},
                upsert=True
            )
            if result.upserted_id or result.modified_count:
                inserted += 1

        print(f"\n ì „ì²´ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {total}ê±´")
        print(f" ê³ ìœ  post_id ìˆ˜: {len(unique_post_ids)}ê±´")
        print(f" ì €ì¥ ë˜ëŠ” ê°±ì‹ : {inserted}ê±´")
        print(f" post_id ëˆ„ë½ìœ¼ë¡œ ìŠ¤í‚µëœ ê±´ìˆ˜: {skipped}ê±´")

    except Exception as e:
        print(f"[MongoDB ì˜¤ë¥˜] {e}")

# ======================
# [6] ë©”ì¸ í•¨ìˆ˜: ë³‘ë ¬ ìˆ˜ì§‘ ë° ì €ì¥ ì‹¤í–‰
# ======================
if __name__ == "__main__":
    categories = get_category_links()  # ì¹´í…Œê³ ë¦¬ ë§í¬ ìˆ˜ì§‘
    print(f"\nğŸ“š ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì™„ë£Œ: {len(categories)}ê°œ")

    total_articles = []  # ì „ì²´ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    lock = threading.Lock()  # ë™ê¸°í™”ìš© ë½ ê°ì²´

    def process_category(cat_url):  # ê° ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ í•¨ìˆ˜
        articles = collect_articles_from_category(cat_url, max_pages=3)
        print(f"  {cat_url} â†’ {len(articles)}ê±´ ìˆ˜ì§‘ë¨")
        with lock:  # ë™ê¸°í™”ëœ ë¦¬ìŠ¤íŠ¸ ì ‘ê·¼
            total_articles.extend(articles)

    with ThreadPoolExecutor(max_workers=5) as executor:
        executor.map(process_category, categories)  # ë³‘ë ¬ ìˆ˜ì§‘ ì‹¤í–‰

    print(f"\nğŸ“° ì´ ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(total_articles)}")
    save_to_mongodb(total_articles)  # MongoDB ì €ì¥