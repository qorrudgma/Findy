import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pymongo import MongoClient
from datetime import datetime
import time

# ======================
# [1] ë©”ì¸ í˜ì´ì§€ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§í¬ ì¶”ì¶œ
# ======================
def get_category_links():
    url = "https://www.chosun.com/"
    headers = {"User-Agent": "Mozilla/5.0"}  # ë´‡ ì°¨ë‹¨ íšŒí”¼ìš© User-Agent ì„¤ì •

    try:
        res = requests.get(url, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        category_links = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©

        # ëª¨ë“  <a> íƒœê·¸ ì¤‘ ì¹´í…Œê³ ë¦¬ ë§í¬ë§Œ í•„í„°ë§
        for a_tag in soup.select("a"):
            href = a_tag.get("href")
            if href and href.startswith("/"):
                full_url = urljoin(url, href)  # ìƒëŒ€ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜

                # ì£¼ìš” ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œê°€ URLì— í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ìˆ˜ì§‘
                if any(cat in href for cat in [
                    "/politics/", "/economy/", "/sports/", "/culture/",
                    "/international/", "/national/", "/opinion/",
                    "/entertainments/", "/game/", "/health/", "/sisa/"
                ]):
                    category_links.add(full_url)

        return list(category_links)  # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ë°˜í™˜

    except Exception as e:
        print(f"[ì—ëŸ¬] ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

# ======================
# [2] ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©, ë³¸ë¬¸, ID, ì‘ì„±ì¼ ì¶”ì¶œ
# ======================
def extract_article_data(article_url):
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(article_url, headers=headers)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # ì œëª©: og:title ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
        title_tag = soup.select_one("meta[property='og:title']")
        title = title_tag["content"] if title_tag else ""

        # ë³¸ë¬¸: section.article-body ë‚´ë¶€ì˜ <p> íƒœê·¸ í…ìŠ¤íŠ¸ë“¤ì„ ëª¨ë‘ ì´ì–´ë¶™ì„
        content_section = soup.select_one("section.article-body")
        content = "" #ì´ˆê¸°ê°’ì„ ì–¸
        if content_section:
            # paragraphs = content_section.find_all("p")
            paragraphs = content_section.select("p.article-body__content.article-body__content-text.text--black")
            content = "\n".join(p.get_text(strip=True) for p in paragraphs)

        # ì‘ì„±ì¼: article:published_time ë©”íƒ€ íƒœê·¸ì—ì„œ ì¶”ì¶œ
        pub_time = soup.select_one("meta[property='article:published_time']")
        published_at = pub_time["content"] if pub_time else None

        # ê²Œì‹œê¸€ ID: URLì˜ ë§ˆì§€ë§‰ segment ì¶”ì¶œ
        parsed = urlparse(article_url)
        post_id = parsed.path.rstrip("/").split("/")[-1]

        return {
            "title": title,
            "link": article_url,
            "content": content,
            "post_id": post_id,
            "published_at": published_at,
            "scraped_at": datetime.utcnow()
        }

    except Exception as e:
        print(f"[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨] {article_url} â†’ {e}")
        return None


# ======================
# [3] ì¹´í…Œê³ ë¦¬ ë‚´ ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ê¸°ì‚¬ ìˆ˜ì§‘
# ======================
def collect_articles_from_category(category_url, max_pages=10):
    headers = {"User-Agent": "Mozilla/5.0"}
    collected = []  # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸

    for page in range(1, max_pages + 1):
        page_url = f"{category_url}?page={page}"
        print(f" [ì¹´í…Œê³ ë¦¬ ìˆ˜ì§‘ ì¤‘] {page_url}")

        try:
            res = requests.get(page_url, headers=headers)
            if res.status_code == 404:
                break  # í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

            soup = BeautifulSoup(res.text, "html.parser")
            links = soup.select("a.story-card__headline")

            if not links:
                print(" ê¸°ì‚¬ ì—†ìŒ, ì¤‘ë‹¨")
                break

            for a_tag in links:
                href = a_tag.get("href")
                if not href:
                    continue

                article_url = urljoin(category_url, href)
                article_data = extract_article_data(article_url)
                if article_data:
                    collected.append(article_data)

            time.sleep(0.5)  # ë„ˆë¬´ ë¹ ë¥´ê²Œ ìš”ì²­í•˜ì§€ ì•Šë„ë¡ ë”œë ˆì´

        except Exception as e:
            print(f"[í˜ì´ì§€ ì˜¤ë¥˜] {page_url} â†’ {e}")
            continue

    return collected

# ======================
# [4] MongoDBì— ì €ì¥
# ======================
def save_to_mongodb(articles):
    try:
        client = MongoClient("mongodb://localhost:27017/")
        db = client["newsdata"]  # DBëª…: newsdata
        collection = db["newsdata"]  # ì»¬ë ‰ì…˜ëª…: newsdata

        count = 0
        for article in articles:
            # post_idê°€ ì—†ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš°ëŠ” ê±´ë„ˆëœ€
            if not article.get("post_id"):
                print(f" post_id ëˆ„ë½: {article.get('title')} - {article.get('link')}")
                continue

            # post_id ê¸°ì¤€ìœ¼ë¡œ upsert (ìˆìœ¼ë©´ ê°±ì‹ , ì—†ìœ¼ë©´ ì‚½ì…)
            # collection.update_one(
            #     {"post_id": article["post_id"]},
            #     {"$set": article},
            #     upsert=True
            # )

            #í…ŒìŠ¤íŠ¸ìš©ë„ë¡œ ì¤‘ë³µí—ˆìš© 
            collection.insert_one(article)

            count += 1

        print(f" MongoDB ì €ì¥ ì™„ë£Œ: {count}ê±´")

    except Exception as e:
        print(f"[MongoDB ì˜¤ë¥˜] {e}")

# ======================
# [5] ì „ì²´ ì‹¤í–‰ íë¦„
# ======================
if __name__ == "__main__":
    categories = get_category_links()  # ë©”ì¸ í˜ì´ì§€ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§í¬ ìˆ˜ì§‘
    print(f"\nğŸ“š ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(categories)}")

    total_saved = 0  # ì „ì²´ ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜

    for cat_url in categories:
        print(f"\n============================")
        print(f" ì¹´í…Œê³ ë¦¬ ì‹œì‘: {cat_url}")
        articles = collect_articles_from_category(cat_url, max_pages=5)
        print(f" ìˆ˜ì§‘ëœ ê¸°ì‚¬ ìˆ˜: {len(articles)}")

        save_to_mongodb(articles)  # ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì €ì¥
        total_saved += len(articles)

    print(f"\n ì „ì²´ ì €ì¥ ì™„ë£Œ: {total_saved}ê±´")
