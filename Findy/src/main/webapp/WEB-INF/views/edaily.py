# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# import hashlib
import time
import traceback

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from urllib.parse import urljoin
# from pymongo import MongoClient
from konlpy.tag import Komoran
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict
from mongo_save import save_to_mongodb
from textrank import textrank_keywords, textrank_summarize

# ì¼ë°˜ ê¸°ì‚¬ì—ì„œ ë³¸ë¬¸ê³¼ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_article_content(article_url):
    try:
        driver.get(article_url)
        time.sleep(1.5)
        soup = BeautifulSoup(driver.page_source, "html.parser")

        # ë³¸ë¬¸ ì¶”ì¶œ
        content_tag = soup.select_one("#news_body_area")
        content = content_tag.get_text(strip=True) if content_tag else "[ë³¸ë¬¸ ì—†ìŒ]"

        # ë‚ ì§œ ì¶”ì¶œ
        date_tag = soup.select_one("div.dates ul li p")
        published_at = "Unknown"

        if date_tag:
            raw_text = date_tag.get_text(strip=True)
            published_at = raw_text  # âœ… ë‚ ì§œ í…ìŠ¤íŠ¸ ì‹¤ì œ ì €ì¥
        else:
            print("â— ë‚ ì§œ íƒœê·¸ ì—†ìŒ")

        return content, published_at

    except Exception as e:
        print(f"âŒ ë³¸ë¬¸/ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return "[ë³¸ë¬¸ ì°¾ê¸° ì‹¤íŒ¨]", "Unknown"



# Selenium ì„¤ì •
options = Options()
options.add_argument("--headless")  # ë©”ë‰´ë¡œì—ì„œ í™”ë©´ ë¹„ì–´ìˆëŠ” ë°©ì‹
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")
driver = webdriver.Chrome(options=options)

# ê¸°ì–µí•´ì•¼ í•  headers (requestsì—ì„œ ìš©ë„)
headers = {
    "User-Agent": "Mozilla/5.0"
}

# ì¹´í…Œê³ ë¦¬ URL ì •ì˜
category_urls = {
    "ê²½ì œ": "https://www.edaily.co.kr/article/economy",
    "ì˜¤í”¼ë‹ˆì–¸": "https://www.edaily.co.kr/opinion",
    "ì‚¬íšŒ": "https://www.edaily.co.kr/article/society",
    "ìŠ¤í¬ì¸ ": "https://www.edaily.co.kr/article/sports",
    "ë¬¸í™”/ì—°ì˜ˆ": "https://www.edaily.co.kr/article/entertainment",
    "ë¬¸í™”/ì—°ì˜ˆ": "https://www.edaily.co.kr/article/culture",
    # "ì •ì¹˜": "https://www.edaily.co.kr/article/politics",
    # "ì¦ê¶Œ": "https://www.edaily.co.kr/article/stock",
    # "ë¶€ë™ì‚°": "https://www.edaily.co.kr/article/estate",
    # "í¬í† ": "https://www.edaily.co.kr/photo/"
}

# ì´ ê¸°ì‚¬ ëª©ë¡
all_articles = []

# ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘
for category, url in category_urls.items():
    print(f"\nâ–¶ ì¹´í…Œê³ ë¦¬: {category} ({url})")
    try:
        driver.get(url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        visited = set()

        # ì˜¤í”¼ë‹ˆì–¸: newsbox / ê¸°íƒ€: newsbox_04
        box_selector = "div.newsbox" if category == "ì˜¤í”¼ë‹ˆì–¸" else "div.newsbox_04"
        articles = soup.select(box_selector)

        for article in articles:
            a_tag = article.select_one("a")
            if not a_tag or not a_tag.get("href"):
                continue

            full_url = urljoin("https://www.edaily.co.kr", a_tag.get("href"))
            if full_url in visited:
                continue
            visited.add(full_url)

            if category == "ì˜¤í”¼ë‹ˆì–¸":
                li_tags = article.select("ul.newsbox_texts li")
                title = li_tags[0].get_text(strip=True) if len(li_tags) > 0 else "[ì œëª© ì—†ìŒ]"
                summary = li_tags[1].get_text(strip=True) if len(li_tags) > 1 else "[ìš”ì•½ ì—†ìŒ]"
            else:
                title_tag = article.select_one("ul.newsbox_texts li")
                title = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"
                summary = a_tag.get_text(strip=True)

            content, published_at = extract_article_content(full_url)
            # content, published_at = extract_article_content(full_url)
            # post_id = hashlib.sha1(full_url.encode("utf-8")).hexdigest()

            # í˜•íƒœì†Œ
            komoran = Komoran()
            pos_result = komoran.pos(content)
            nouns = [word for word, tag in pos_result if tag in ['NNG', 'NNP'] and len(word) > 1]

            sentences = content.split('.')
            first_sentence = sentences[1] if len(sentences) > 1 else ""
            last_sentence = sentences[-1]
            position_weights = defaultdict(float)

            for word, tag in pos_result:
                if len(word) <= 1 or tag not in ['NNG', 'NNP']:
                    continue
                if word in title:
                    position_weights[word] += 2.0
                if word in first_sentence:
                    position_weights[word] += 1.5
                if word in last_sentence:
                    position_weights[word] += 1.0
                if tag == 'NNP':
                    position_weights[word] += 1.0

            # TF-IDF
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform([" ".join(nouns)])
            feature_names = vectorizer.get_feature_names_out()
            tfidf_scores = tfidf_matrix.toarray()[0]

            tfidf_keywords = []
            for word, score in zip(feature_names, tfidf_scores):
                final_score = score + position_weights.get(word, 0)
                tfidf_keywords.append((word, final_score))
            tfidf_keywords = sorted(tfidf_keywords, key=lambda x: x[1], reverse=True)

            # TextRank
            textrank_kw = textrank_keywords(nouns)

            # ì¤‘ìš” ë‚´ìš©
            sentences = [s.strip() for s in summary.split('.') if len(s.strip()) > 10]
            summary_sentences = textrank_summarize(sentences, top_k=3)

            doc = {
                # "id": post_id,
                "headline": title,
                "content": summary,
                "url": full_url,
                "category": category,
                "time": published_at,
                "summary": summary_sentences,
                "tfidf_keywords": tfidf_keywords[:10],
                "textrank_keywords": textrank_kw,
                "source": "edaily"
            }

            all_articles.append(doc)
            print(f"ğŸ“° {title}")
            time.sleep(0.2)

    except Exception as e:
        print(f"âŒ [{category}] ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()

# ë°”ë¡œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œí•˜ê³  ëª¨ë“  ê²ƒ ì €ì¥
driver.quit()
save_to_mongodb(all_articles)