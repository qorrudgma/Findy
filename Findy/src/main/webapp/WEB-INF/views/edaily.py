# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pymongo import MongoClient
import requests
import hashlib
import time
import traceback

# MongoDBì— ê¸°ì‚¬ë“¤ì„ ì €ì¥í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
def save_to_mongodb(articles):
    try:
        client = MongoClient("mongodb://localhost:27017/")  # MongoDB í´ë¼ì´ì–¸íŠ¸ ì—°ê²°
        db = client["newsdata1"]                             # DB ì´ë¦„
        collection = db["newsdata1"]                         # ì»¬ë ‰ì…˜ ì´ë¦„

        total = len(articles)  # ì „ì²´ ê¸°ì‚¬ ìˆ˜
        unique_post_ids = set()  # ì¤‘ë³µ ì œê±°ìš© post_id ì§‘í•©
        inserted = 0
        skipped = 0

        for article in articles:
            post_id = article["post_id"]
            if post_id in unique_post_ids:
                skipped += 1
                continue

            if collection.count_documents({"post_id": post_id}, limit=1) == 0:
                collection.insert_one(article)
                inserted += 1
            else:
                skipped += 1

            unique_post_ids.add(post_id)

        print(f"\nğŸ“¢ MongoDB ì €ì¥ ì™„ë£Œ | ì´ ì‹œë„: {total}, ì‚½ì…: {inserted}, ì¤‘ë³µ ìŠ¤í‚µ: {skipped}")
    except Exception as e:
        print(f"âŒ MongoDB ì €ì¥ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

# ì¼ë°˜ ê¸°ì‚¬ì—ì„œ ë³¸ë¬¸ê³¼ ë‚ ì§œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_article_content(article_url):
    try:
        res = requests.get(article_url, headers=headers)
        soup = BeautifulSoup(res.text, "html.parser")
        content_tag = soup.select_one("#news_body_area")
        content = content_tag.get_text(strip=True) if content_tag else "[ë¶„ë¬¸ ì—†ìŒ]"

        date_tag = soup.select_one("div.article_info span.date")
        published_at = date_tag.get_text(strip=True) if date_tag else "Unknown"

        return content, published_at
    except Exception as e:
        return f"[ë¶„ë¬¸ ì°¾ê¸° ì‹¤íŒ¨] {e}", "Unknown"

# Selenium ì„¤ì •
options = Options()
options.add_argument("--headless")  # ë©”ë‰´ë¡œì—ì„œ í™”ë©´ ë¹„ì–´ìˆëŠ” ë°©ì‹
options.add_argument("--disable-gpu")
options.add_argument("--window-size=1920,1080")
driver = webdriver.Chrome(options=options)

# ê¸°ì–µí•´ì•¼ í•  headers (requestsì—ì„œ ìš©ë„)
headers = {
    "User-Agent": "Mozilla/5.0"
}

# ì¹´í…Œê³ ë¦¬ URL ì •ì˜
category_urls = {
    "ê²½ì œ": "https://www.edaily.co.kr/article/economy",
    "ì •ì¹˜": "https://www.edaily.co.kr/article/politics",
    "ì‚¬íšŒ": "https://www.edaily.co.kr/article/society",
    "ì¦ê¶Œ": "https://www.edaily.co.kr/article/stock",
    "ë¶€ë™ì‚°": "https://www.edaily.co.kr/article/estate",
    "ë¬¸í™”": "https://www.edaily.co.kr/article/culture",
    "ì—°ì˜ˆ": "https://www.edaily.co.kr/article/entertainment",
    "ìŠ¤í¬ì¸ ": "https://www.edaily.co.kr/article/sports",
    "ì˜¤í”¼ë‹ˆì–¸": "https://www.edaily.co.kr/opinion",
    "í¬í† ": "https://www.edaily.co.kr/photo/"
}

# ì´ ê¸°ì‚¬ ëª©ë¡
all_articles = []

# ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘
for category, url in category_urls.items():
    print(f"\nâ–¶ ì¹´í…Œê³ ë¦¬: {category} ({url})")
    try:
        driver.get(url)
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, "html.parser")
        visited = set()

        # í¬í†  ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
        if category == "í¬í† ":
            photo_section = soup.select_one("div.projects.clearfix.cols-3")
            if not photo_section:
                print("âŒ [í¬í† ] projects ì˜ì—­ ì—†ìŒ")
                continue

            photo_items = photo_section.select("div.project-cont")
            print(f"ğŸ” ë°œê²¬ëœ ì‚¬ì§„ ê°œìˆ˜: {len(photo_items)}")

            for idx, item in enumerate(photo_items, start=1):
                try:
                    title_tag = item.select_one("div.overlay h5")
                    img_tag = item.select_one("img")

                    title = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"
                    image_url = urljoin(url, img_tag["src"]) if img_tag and img_tag.get("src") else "[ì´ë¯¸ì§€ ì—†ìŒ]"

                    date_tag = item.select_one("span.date") or item.select_one("div.date")
                    published_at = date_tag.get_text(strip=True) if date_tag else ""

                    post_id = hashlib.sha1(image_url.encode("utf-8")).hexdigest()

                    doc = {
                        "post_id": post_id,
                        "title": title,
                        "image_url": image_url,
                        "url": url,
                        "published_at": published_at,
                        "source": "edaily.co.kr",
                        "category": category
                    }

                    all_articles.append(doc)
                    print(f"{idx}. ğŸ–¼ï¸ {title}")
                    time.sleep(0.2)

                except Exception as inner_e:
                    print(f"âŒ [{idx}] ì‚¬ì§„ ì°¾ê¸° ì‹¤íŒ¨: {inner_e}")
                    traceback.print_exc()

        else:
            # ì˜¤í”¼ë‹ˆì–¸: newsbox / ê¸°íƒ€: newsbox_04
            box_selector = "div.newsbox" if category == "ì˜¤í”¼ë‹ˆì–¸" else "div.newsbox_04"
            articles = soup.select(box_selector)

            for article in articles:
                a_tag = article.select_one("a")
                if not a_tag or not a_tag.get("href"):
                    continue

                full_url = urljoin("https://www.edaily.co.kr", a_tag.get("href"))
                if full_url in visited:
                    continue
                visited.add(full_url)

                if category == "ì˜¤í”¼ë‹ˆì–¸":
                    li_tags = article.select("ul.newsbox_texts li")
                    title = li_tags[0].get_text(strip=True) if len(li_tags) > 0 else "[ì œëª© ì—†ìŒ]"
                    summary = li_tags[1].get_text(strip=True) if len(li_tags) > 1 else "[ìš”ì•½ ì—†ìŒ]"
                else:
                    title_tag = article.select_one("ul.newsbox_texts li")
                    title = title_tag.get_text(strip=True) if title_tag else "[ì œëª© ì—†ìŒ]"
                    summary = a_tag.get_text(strip=True)

                content, published_at = extract_article_content(full_url)
                post_id = hashlib.sha1(full_url.encode("utf-8")).hexdigest()

                doc = {
                    "post_id": post_id,
                    "title": title,
                    "summary": summary,
                    "content": summary,  # ë¶„ë¬¸ ëŒ€ì‹  ìš”ì•½ ë“œë¦¼
                    "url": full_url,
                    "category": category,
                    "published_at": published_at,
                    "source": "edaily.co.kr"
                }

                all_articles.append(doc)
                print(f"ğŸ“° {title}")
                time.sleep(0.2)

    except Exception as e:
        print(f"âŒ [{category}] ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()

# ë°”ë¡œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œí•˜ê³  ëª¨ë“  ê²ƒ ì €ì¥
driver.quit()
save_to_mongodb(all_articles)